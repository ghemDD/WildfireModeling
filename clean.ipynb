{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wildfire susceptibility\n",
    "\n",
    "**Abstract**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries & Data Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general usage ML libraries \n",
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_rows', 25)\n",
    "\n",
    "# vector operations\n",
    "import geopandas as gpd\n",
    "import shapely.geometry\n",
    "import shapely.wkt\n",
    "\n",
    "# raster operations\n",
    "import os\n",
    "from osgeo import gdal, osr\n",
    "import rasterio.mask \n",
    "import rasterstats\n",
    "import rasterio \n",
    "from rasterio.plot import show\n",
    "from rasterio.warp import reproject, Resampling, calculate_default_transform\n",
    "from rasterio.features import rasterize\n",
    "\n",
    "import subprocess\n",
    "\n",
    "# CRS transformations\n",
    "import pyproj\n",
    "\n",
    "# interpretability\n",
    "import shap\n",
    "\n",
    "# visualization\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data_final/'\n",
    "\n",
    "# Land cover data\n",
    "LC_YEAR = [1990, 2000, 2006, 2012, 2018]\n",
    "LC_PATHS = [DATA_PATH+'lc/LC_'+str(year)+'_ND_REC.tif' for year in LC_YEAR]\n",
    "\n",
    "# distance to urban areas\n",
    "DISTANCE_PATHS = [DATA_PATH+'distance/URB_DIST_'+str(year)+'.tif' for year in LC_YEAR]\n",
    "\n",
    "# most recent fires year on top\n",
    "FIREYEAR = 'data_final/fires/FIREYEAR.tif'\n",
    "\n",
    "# fire binary variable per time period\n",
    "FIRE_YEAR = ['2001-2005', '2012-2017', '2018', '2019', '2020', '2021', '2022']\n",
    "YEAR_PATHS = ['FIRE_ANTE_2006.tif', 'FIRE_2012_TO_2018.tif', 'FIRE_2018.tif', 'FIRE_2019.tif', 'FIRE_2020.tif', \n",
    "                  'FIRE_2021.tif', 'FIRE_2022.tif']\n",
    "\n",
    "FIREYEAR_PATHS = ['data_final/fires/'+year_path for year_path in YEAR_PATHS]\n",
    "\n",
    "# topographic data\n",
    "DEM_PATH = 'data_final/topographic/dem.tif'\n",
    "SLOPE_PATH = 'data_final/topographic/SLOPE_ND.tif'\n",
    "ASPECT_PATH = 'data_final/topographic/aspect.tif'\n",
    "HILLSHADE_PATH = 'data_final/topographic/hillshade.tif'\n",
    "\n",
    "# protected areas\n",
    "PROTECTED_PATH = 'data_final/protected/raster_natural_protected.tif'\n",
    "\n",
    "# vegetation seasonal data\n",
    "VEGETATION_YEAR = ['2017', '2018', '2019', '2020', '2021']\n",
    "VEGETATION_SEASON = ['season1', 'season2']\n",
    "VEGETATION_VARS = ['QFLAG', 'MINV', 'MAXV', 'AMPL', 'RSLOPE', 'LSLOPE']\n",
    "VEGETATION_PATHS = ['data_final/season/'+year+'/merged_'+year+'_'+season+'_'+var+'.tif' for year in VEGETATION_YEAR for var in VEGETATION_VARS\n",
    "                   for season in VEGETATION_SEASON]\n",
    "\n",
    "VEG_IND_VARS = ['ndvi', 'vi', 'evi']\n",
    "VEG_IND_YEAR = VEGETATION_YEAR[1:] + ['2022']\n",
    "\n",
    "\n",
    "# population density\n",
    "DENSITY_PATH = 'data_final/population/population_density_mode.tif'\n",
    "\n",
    "# distance to populated areas\n",
    "DENSITY_VALUES = ['2', '3', '4']\n",
    "\n",
    "# emissivity and land surface temperature\n",
    "TEMP_YEAR = ['2018', '2019', '2020', '2021', '2022']\n",
    "EMISSIVITY_PATHS = ['data_final/emissivity/emissivity_'+year+'.tif' for year in TEMP_YEAR]\n",
    "LST_PATHS = ['data_final/emissivity/lst_'+year+'.tif' for year in TEMP_YEAR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_density_raster = {}\n",
    "dist_density_array = {}\n",
    "\n",
    "[dist_density_raster.update({value: rasterio.open('data_final/population/distance_density_'+value+'.tif')}) for value in DENSITY_VALUES]\n",
    "[dist_density_array.update({value: dist_density_raster[value].read(1).flatten()}) for value in DENSITY_VALUES]\n",
    "\n",
    "# vegetation (Nasa)\n",
    "\n",
    "veg_ind_raster = {}\n",
    "veg_ind_array = {}\n",
    "\n",
    "[veg_ind_raster.update({var+'_'+year : rasterio.open('data_final/vegetation/'+var+'_'+year+'.tif')}) for year in VEG_IND_YEAR \n",
    " for var in VEG_IND_VARS]\n",
    "[veg_ind_array.update({var+'_'+year: veg_ind_raster[var+'_'+year].read(1).flatten()}) for year in VEG_IND_YEAR for var in VEG_IND_VARS]\n",
    "\n",
    "# vegetation (wekeo)\n",
    "vegetation_raster = {}\n",
    "vegetation_array = {}\n",
    "\n",
    "[vegetation_raster.update({var+'_'+year+'_'+season : rasterio.open('data_final/season/'+year+'/merged_'+year+'_'+season+'_'+var+'.tif') }) \n",
    " for (i, year) in enumerate(VEGETATION_YEAR) for (y, var) in enumerate(VEGETATION_VARS) for (u, season) in enumerate(VEGETATION_SEASON)]\n",
    "[vegetation_raster.update({var+'_2022_season1': vegetation_raster[var+'_2021_season1']}) for var in VEGETATION_VARS]\n",
    "\n",
    "[vegetation_array.update({var+'_'+year+'_'+season : vegetation_raster[var+'_'+year+'_'+season].read(1).flatten() }) for (i, year) in enumerate(VEGETATION_YEAR) \n",
    " for (y, var) in enumerate(VEGETATION_VARS) for (u, season) in enumerate(VEGETATION_SEASON)]\n",
    "[vegetation_array.update({var+'_2022_season1': vegetation_array[var+'_2021_season1']}) for var in VEGETATION_VARS]\n",
    "\n",
    "\n",
    "# emissivity and land surface temperature rasters\n",
    "emissivity_raster = {}\n",
    "emissivity_array = {}\n",
    "\n",
    "[emissivity_raster.update({str(year) : rasterio.open(EMISSIVITY_PATHS[i]) }) for (i, year) in enumerate(TEMP_YEAR)]\n",
    "[emissivity_array.update({str(year) : emissivity_raster[str(year)].read(1).flatten()}) for (i, year) in enumerate(TEMP_YEAR)]\n",
    "\n",
    "\n",
    "lst_raster = {}\n",
    "lst_array = {}\n",
    "\n",
    "[lst_raster.update({str(year) : rasterio.open(LST_PATHS[i]) }) for (i, year) in enumerate(TEMP_YEAR)]\n",
    "[lst_array.update({str(year) : lst_raster[str(year)].read(1).flatten()}) for (i, year) in enumerate(TEMP_YEAR)]\n",
    "\n",
    "# land cover rasters\n",
    "lc_rasters = {}\n",
    "lc_array = {}\n",
    "\n",
    "[lc_rasters.update({str(year) : rasterio.open(LC_PATHS[i]) }) for (i, year) in enumerate(LC_YEAR)]\n",
    "[lc_array.update({str(year) : lc_rasters[str(year)].read(1).flatten().astype(np.int8) }) for (i, year) in enumerate(LC_YEAR)]\n",
    "\n",
    "# distance to urban facilities\n",
    "dist_rasters = {}\n",
    "dist_array = {}\n",
    "\n",
    "[dist_rasters.update({str(year) : rasterio.open(DISTANCE_PATHS[i]) }) for (i, year) in enumerate(LC_YEAR)]\n",
    "[dist_array.update({str(year) : dist_rasters[str(year)].read(1).flatten() }) for (i, year) in enumerate(LC_YEAR)]\n",
    "\n",
    "\n",
    "# fireyear raster\n",
    "#fireyear_raster = rasterio.open(DATA_PATH+'FIREYEAR.tif')\n",
    "fireyear_raster = {}\n",
    "fireyear_array = {}\n",
    "fireyear_array_bin = {}\n",
    "\n",
    "[fireyear_raster.update({year : rasterio.open(FIREYEAR_PATHS[i]) }) for (i, year) in enumerate(FIRE_YEAR)]\n",
    "[fireyear_array.update({year : fireyear_raster[str(year)].read(1).flatten() }) for (i, year) in enumerate(FIRE_YEAR)]\n",
    "[fireyear_array_bin.update({year : np.array(fireyear_array[str(year)]) > 0 }) for (i, year) in enumerate(FIRE_YEAR)]\n",
    "\n",
    "cumulated_fireyear_raster = rasterio.open(DATA_PATH+'fires/FIREYEAR.tif')\n",
    "cumulated_fireyear_array = cumulated_fireyear_raster.read(1).flatten()\n",
    "\n",
    "cumulated_fireyear_raster_train = rasterio.open(DATA_PATH + 'fires/FIREYEAR_TRAIN.tif')\n",
    "cumulated_fireyear_array_train = cumulated_fireyear_raster_train.read(1).flatten()\n",
    "\n",
    "# topographic variables\n",
    "dem_raster = rasterio.open(DEM_PATH)\n",
    "dem_array = dem_raster.read(1).flatten()\n",
    "\n",
    "slope_raster = rasterio.open(SLOPE_PATH)\n",
    "slope_array = slope_raster.read(1).flatten()\n",
    "\n",
    "aspect_raster = rasterio.open(ASPECT_PATH)\n",
    "aspect_array = aspect_raster.read(1).flatten()\n",
    "\n",
    "hillshade_raster = rasterio.open(HILLSHADE_PATH)\n",
    "hillshade_array = hillshade_raster.read(1).flatten()\n",
    "\n",
    "# protected areas\n",
    "protected_raster = rasterio.open(PROTECTED_PATH)\n",
    "protected_array = protected_raster.read(1).flatten()\n",
    "\n",
    "# population density\n",
    "density_raster = rasterio.open(DENSITY_PATH)\n",
    "density_array = density_raster.read(1).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame(data=np.c_[\n",
    "                                  # land cover\n",
    "                                  lc_array['1990'], lc_array['2000'], lc_array['2006'], \n",
    "                                  lc_array['2012'], lc_array['2018'],\n",
    "                                  \n",
    "                                  # topographic variables\n",
    "                                  slope_array, dem_array, protected_array, \n",
    "                                  aspect_array, hillshade_array,\n",
    "                                  \n",
    "                                  # distance to artificial surfaces\n",
    "                                  dist_array['1990'], \n",
    "                                  dist_array['2000'], \n",
    "                                  dist_array['2006'], \n",
    "                                  dist_array['2012'], \n",
    "                                  dist_array['2018'], \n",
    "                                  \n",
    "                                  # fire binary per year\n",
    "                                  fireyear_array_bin['2001-2005'], \n",
    "                                  fireyear_array_bin['2012-2017'], \n",
    "                                  fireyear_array_bin['2018'], \n",
    "                                  fireyear_array_bin['2019'], \n",
    "                                  fireyear_array_bin['2020'], \n",
    "                                  fireyear_array_bin['2021'],\n",
    "                                  \n",
    "                                  # cumulated fireyear\n",
    "                                  cumulated_fireyear_array_train, \n",
    "                                  \n",
    "                                  \n",
    "                                  # vegetation data (Wekeo)\n",
    "                                  vegetation_array['QFLAG_2017_season1'], vegetation_array['MINV_2017_season1'], \n",
    "                                  vegetation_array['MAXV_2017_season1'], vegetation_array['RSLOPE_2017_season1'], \n",
    "                                  vegetation_array['LSLOPE_2017_season1'], vegetation_array['AMPL_2017_season1'], \n",
    "                       \n",
    "                                  vegetation_array['QFLAG_2018_season1'], vegetation_array['MINV_2018_season1'], \n",
    "                                  vegetation_array['MAXV_2018_season1'], vegetation_array['RSLOPE_2018_season1'], \n",
    "                                  vegetation_array['LSLOPE_2018_season1'], vegetation_array['AMPL_2018_season1'],\n",
    "                                  \n",
    "                                  \n",
    "                                  vegetation_array['QFLAG_2019_season1'], vegetation_array['MINV_2019_season1'], \n",
    "                                  vegetation_array['MAXV_2019_season1'], vegetation_array['RSLOPE_2019_season1'], \n",
    "                                  vegetation_array['LSLOPE_2019_season1'], vegetation_array['AMPL_2019_season1'],\n",
    "                                  \n",
    "                                  vegetation_array['QFLAG_2020_season1'], vegetation_array['MINV_2020_season1'], \n",
    "                                  vegetation_array['MAXV_2020_season1'], vegetation_array['RSLOPE_2020_season1'], \n",
    "                                  vegetation_array['LSLOPE_2020_season1'], vegetation_array['AMPL_2020_season1'],\n",
    "                                  \n",
    "                                  vegetation_array['QFLAG_2021_season1'], vegetation_array['MINV_2021_season1'], \n",
    "                                  vegetation_array['MAXV_2021_season1'], vegetation_array['RSLOPE_2021_season1'], \n",
    "                                  vegetation_array['LSLOPE_2021_season1'], vegetation_array['AMPL_2021_season1'], \n",
    "                                  \n",
    "                                  \n",
    "                                  lst_array['2018'], lst_array['2019'], lst_array['2020'], lst_array['2021'],\n",
    "                                  emissivity_array['2018'], emissivity_array['2019'], emissivity_array['2020'], \n",
    "                                  emissivity_array['2021'], veg_ind_array['ndvi_2018'], veg_ind_array['ndvi_2019'],\n",
    "                                  veg_ind_array['ndvi_2020'], veg_ind_array['ndvi_2021'], \n",
    "                                  \n",
    "                                  veg_ind_array['evi_2018'], veg_ind_array['evi_2019'],\n",
    "                                  veg_ind_array['evi_2020'], veg_ind_array['evi_2021'],\n",
    "                                  \n",
    "                                  veg_ind_array['vi_2018'], veg_ind_array['vi_2019'],\n",
    "                                  veg_ind_array['vi_2020'], veg_ind_array['vi_2021']\n",
    "                                 ])\n",
    "\n",
    "\n",
    "columns_df = [\n",
    "              # land cover\n",
    "              'LC90', 'LC00', 'LC06', 'LC12', 'LC18', \n",
    "              \n",
    "              # topographic variables\n",
    "              'slope', 'dem', 'protected', 'aspect', 'hillshade',\n",
    "              \n",
    "              # distance to artificial surfaces\n",
    "              'URB90', 'URB00', 'URB06', 'URB12', 'URB18',\n",
    "              \n",
    "              # fire per year binary\n",
    "              'F/2001-2005', 'F/2012-2017', 'F/2018', 'F/2019', 'F/2020', 'F/2021', \n",
    "              \n",
    "              # cumulated fireyear\n",
    "              'fire',  \n",
    "              \n",
    "              # vegetation data (wekeo)\n",
    "              'QFLAG_2017_s1', 'MINV_2017_s1', 'MAXV_2017_s1', 'RSLOPE_2017_s1', 'LSLOPE_2017_s1','AMPL_2017_s1',\n",
    "              'QFLAG_2018_s1', 'MINV_2018_s1', 'MAXV_2018_s1', 'RSLOPE_2018_s1', 'LSLOPE_2018_s1','AMPL_2018_s1',\n",
    "              'QFLAG_2019_s1', 'MINV_2019_s1', 'MAXV_2019_s1', 'RSLOPE_2019_s1', 'LSLOPE_2019_s1','AMPL_2019_s1',\n",
    "              'QFLAG_2020_s1', 'MINV_2020_s1', 'MAXV_2020_s1', 'RSLOPE_2020_s1', 'LSLOPE_2020_s1','AMPL_2020_s1',\n",
    "               'QFLAG_2021_s1', 'MINV_2021_s1', 'MAXV_2021_s1', 'RSLOPE_2021_s1', 'LSLOPE_2021_s1','AMPL_2021_s1', \n",
    "              \n",
    "              # land surface temperature\n",
    "              'LST_2018', 'LST_2019', 'LST_2020', 'LST_2021', \n",
    "    \n",
    "              # emissivity\n",
    "              'EMI_2018', 'EMI_2019','EMI_2020', 'EMI_2021', \n",
    "    \n",
    "              # extra vegetation data\n",
    "              'NDVI_2018', 'NDVI_2019', 'NDVI_2020', 'NDVI_2021', \n",
    "              'EVI_2018', 'EVI_2019', 'EVI_2020', 'EVI_2021', \n",
    "              'VI_2018', 'VI_2019', 'VI_2020', 'VI_2021', \n",
    "            ]\n",
    "\n",
    "df_data.columns = columns_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to select the Land cover corresponding to the fire year\n",
    "\n",
    "def find_year_lc(fire_year, df, column):\n",
    "    # Value of fire = 0 corresponds to no fire for the region\n",
    "    if fire_year==0 : \n",
    "        return df[column+'18']\n",
    "    elif fire_year<2000 :\n",
    "        return df[column+'90']\n",
    "    elif fire_year<2006 :\n",
    "        return df[column+'00']\n",
    "    elif fire_year<2012 : \n",
    "        return df[column+'06']\n",
    "    elif fire_year<2018 :\n",
    "        return df[column+'12']\n",
    "    else : \n",
    "        return df[column+'18']\n",
    "    \n",
    "def find_year_vegetation(fire_year, df, column):\n",
    "    # \n",
    "    if fire_year==0 : \n",
    "        return df[column+'_2021_s1']\n",
    "    elif fire_year<=2017 :\n",
    "        return df[column+'_2017_s1']\n",
    "    elif fire_year==2018 :\n",
    "        return df[column+'_2018_s1']\n",
    "    elif fire_year==2019 :\n",
    "        return df[column+'_2019_s1']\n",
    "    elif fire_year==2020 : \n",
    "        return df[column+'_2020_s1']\n",
    "    elif fire_year==2021 :\n",
    "        return df[column+'_2021_s1']\n",
    "    else : \n",
    "        return df[column+'_2021_s1']\n",
    "    \n",
    "def find_year_emi_ind(fire_year, df, column):\n",
    "    if fire_year==0 : \n",
    "        return df[column+'_2021']\n",
    "    elif fire_year<=2018 :\n",
    "        return df[column+'_2018']\n",
    "    elif fire_year==2019 :\n",
    "        return df[column+'_2019']\n",
    "    elif fire_year==2020 : \n",
    "        return df[column+'_2020']\n",
    "    elif fire_year==2021 :\n",
    "        return df[column+'_2021']\n",
    "    else : \n",
    "        return df[column+'_2021']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Selecting the Land cover and distance to artifical surfaces corresponding to the fire year\n",
    "df_data['lc'] = df_data.apply(lambda x: find_year_lc(x['fire'], x, 'LC'), axis = 1)\n",
    "df_data['urb'] = df_data.apply(lambda x: find_year_lc(x['fire'], x, 'URB'), axis = 1)\n",
    "\n",
    "drop_columns = ['LC90', 'LC00', 'LC06', 'LC12', 'LC18', 'URB90', 'URB00', 'URB06', 'URB12', 'URB18']\n",
    "\n",
    "df_data = df_data.drop(columns=drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Selecting vegetation indices variables corresponding to the fire year\n",
    "df_data['qflag'] = df_data.apply(lambda x: find_year_vegetation(x['fire'], x, 'QFLAG'), axis = 1)\n",
    "df_data['ampl'] = df_data.apply(lambda x: find_year_vegetation(x['fire'], x, 'AMPL'), axis = 1)\n",
    "df_data['minv'] = df_data.apply(lambda x: find_year_vegetation(x['fire'], x, 'MINV'), axis = 1)\n",
    "df_data['maxv'] = df_data.apply(lambda x: find_year_vegetation(x['fire'], x, 'MAXV'), axis = 1)\n",
    "df_data['rslope'] = df_data.apply(lambda x: find_year_vegetation(x['fire'], x, 'RSLOPE'), axis = 1)\n",
    "df_data['lslope'] = df_data.apply(lambda x: find_year_vegetation(x['fire'], x, 'LSLOPE'), axis = 1)\n",
    "\n",
    "drop_columns = ['QFLAG_2017_s1', 'MINV_2017_s1', 'MAXV_2017_s1', 'RSLOPE_2017_s1', 'LSLOPE_2017_s1',\n",
    "                  'AMPL_2017_s1',\n",
    "                   'QFLAG_2018_s1', 'MINV_2018_s1', 'MAXV_2018_s1', 'RSLOPE_2018_s1', 'LSLOPE_2018_s1',\n",
    "                  'AMPL_2018_s1',\n",
    "                   'QFLAG_2019_s1', 'MINV_2019_s1', 'MAXV_2019_s1', 'RSLOPE_2019_s1', 'LSLOPE_2019_s1',\n",
    "                  'AMPL_2019_s1',\n",
    "                   'QFLAG_2020_s1', 'MINV_2020_s1', 'MAXV_2020_s1', 'RSLOPE_2020_s1', 'LSLOPE_2020_s1',\n",
    "                  'AMPL_2020_s1',\n",
    "                   'QFLAG_2021_s1', 'MINV_2021_s1', 'MAXV_2021_s1', 'RSLOPE_2021_s1', 'LSLOPE_2021_s1',\n",
    "                  'AMPL_2021_s1']\n",
    "\n",
    "df_data = df_data.drop(columns=drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data['dist_2'] = dist_density_array['2']\n",
    "df_data['dist_3'] = dist_density_array['3']\n",
    "df_data['dist_4'] = dist_density_array['4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_data['emi'] = df_data.apply(lambda x: find_year_emi_ind(x['fire'], x, 'EMI'), axis = 1)\n",
    "df_data['lst'] = df_data.apply(lambda x: find_year_emi_ind(x['fire'], x, 'LST'), axis = 1)\n",
    "\n",
    "drop_columns = ['LST_2018', 'LST_2019', 'LST_2020', 'LST_2021', 'EMI_2018', 'EMI_2019',\n",
    "              'EMI_2020', 'EMI_2021']\n",
    "\n",
    "df_data = df_data.drop(columns=drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_data['ndvi'] = df_data.apply(lambda x: find_year_emi_ind(x['fire'], x, 'NDVI'), axis = 1)\n",
    "df_data['evi'] = df_data.apply(lambda x: find_year_emi_ind(x['fire'], x, 'EVI'), axis = 1)\n",
    "df_data['vi'] = df_data.apply(lambda x: find_year_emi_ind(x['fire'], x, 'VI'), axis = 1)\n",
    "\n",
    "drop_columns = ['NDVI_2018', 'NDVI_2019', 'NDVI_2020', 'NDVI_2021', \n",
    "              'EVI_2018', 'EVI_2019', 'EVI_2020', 'EVI_2021', \n",
    "              'VI_2018', 'VI_2019', 'VI_2020', 'VI_2021']\n",
    "\n",
    "df_data = df_data.drop(columns=drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year of the data of a cell\n",
    "df_data['year'] = df_data.apply(lambda x : 2018 if(x['fire'] == 0) else x['fire'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary variable indicating if cell has burnt\n",
    "df_data['is_fire'] = df_data.apply(lambda x : 1 if(x['fire'] != 0) else 0 , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "threshold_years = [2005, 2012, 2018, 2019, 2020, 2021]\n",
    "drop_columns = ['F/2001-2005', 'F/2012-2017', 'F/2018', 'F/2019', 'F/2020', 'F/2021']\n",
    "\n",
    "def fire_count_year(x):\n",
    "    count = 0\n",
    "    year = x['year']\n",
    "    for col, threshold in zip(drop_columns, threshold_years):\n",
    "        if (year > threshold):\n",
    "            count += x[col]\n",
    "    return count\n",
    "    \n",
    "df_data['fire_count'] = df_data.apply(lambda x: fire_count_year(x), axis = 1)\n",
    "\n",
    "df = df_data.drop(columns=drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_cols = list(pd.get_dummies(df.lc, prefix='lc').columns)\n",
    "\n",
    "name_list = ['Empty', 'Artificial surfaces', 'Wetlands', 'Non-irrigated arable land',\n",
    "            'Vineyards', 'Fruit trees and berry plantations', 'Pastures',\n",
    "            'Complex cultivation patterns', 'Land principally occupied by agriculture with significant areas of natural vegetation', \n",
    "            'Broad-leaved forest', 'Coniferous forest', 'Mixed forest', 'Natural grasslands', \n",
    "            'Moors and heathland', 'Transitional woodland-shrub', 'Beaches - dunes - sands', \n",
    "            'Bare rocks', 'Sparsely vegetated areas', 'Glaciers and perpetual snow']\n",
    "\n",
    "# mapping from column name i.e LC_[grid_code_value] => land cover type\n",
    "legend_column = dict(list(zip(lc_cols, name_list)))\n",
    "\n",
    "# mapping from land cover type => column name i.e LC_[grid_code_value]\n",
    "legend_name = dict(list(zip(name_list, lc_cols)))\n",
    "\n",
    "# we keep the grid value of the land cover for usability reasons\n",
    "df[lc_cols] = pd.get_dummies(df.lc, prefix='lc')\n",
    "\n",
    "# example usage\n",
    "len(df[df[legend_name['Artificial surfaces']] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.slope != 255) & (df.dem != 0.) & (df.lc != 0) & (df.lc != 1) & (df.lc != 2) & (df.lc != 30) & (df.lc != 31) & (df.lc != 32) & (df.lc != 33)\n",
    "               & (df.lc != 34) & (df.minv != -32768) & (df.maxv != -32768)]\n",
    "df_train = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_data_year = {}\n",
    "\n",
    "# block indexing for spatial cross validation \n",
    "h_block_size = 101\n",
    "w_block_size = 129\n",
    "width = 1943\n",
    "height = 1156\n",
    "\n",
    "for y, year in enumerate(FIRE_YEAR[-1:]):\n",
    "    print(year)\n",
    "    land_cover = []\n",
    "    \n",
    "    year_int = int(year[:4])\n",
    "    if year_int < 2018:\n",
    "        if year_int < 2000 :\n",
    "            land_cover = lc_array['1990']\n",
    "            urb = dist_array['1990']\n",
    "\n",
    "        elif year_int < 2006 :\n",
    "            land_cover = lc_array['2000']\n",
    "            urb = dist_array['2000']\n",
    "\n",
    "        elif year_int < 2012 : \n",
    "            land_cover = lc_array['2006']\n",
    "            urb = dist_array['2006']\n",
    "\n",
    "        elif year_int < 2018 :\n",
    "            land_cover = lc_array['2012']\n",
    "            urb = dist_array['2012']\n",
    "        \n",
    "        \n",
    "        qflag = vegetation_array['QFLAG_2017_season1']\n",
    "        minv = vegetation_array['MINV_2017_season1']\n",
    "        maxv = vegetation_array['MAXV_2017_season1']\n",
    "        rslope = vegetation_array['RSLOPE_2017_season1']\n",
    "        lslope = vegetation_array['LSLOPE_2017_season1']\n",
    "        ampl = vegetation_array['AMPL_2017_season1'] \n",
    "        \n",
    "        emi = emissivity_array['2018']\n",
    "        lst = lst_array['2018']\n",
    "        ndvi = veg_ind_array['ndvi_2018']\n",
    "        evi = veg_ind_array['evi_2018']\n",
    "        vi = veg_ind_array['vi_2018']\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        land_cover = lc_array['2018']\n",
    "        urb = dist_array['2018']\n",
    "        \n",
    "        qflag = vegetation_array['QFLAG_'+str(year)+'_season1']\n",
    "        minv = vegetation_array['MINV_'+str(year)+'_season1']\n",
    "        maxv = vegetation_array['MAXV_'+str(year)+'_season1']\n",
    "        rslope = vegetation_array['RSLOPE_'+str(year)+'_season1']\n",
    "        lslope = vegetation_array['LSLOPE_'+str(year)+'_season1']\n",
    "        ampl = vegetation_array['AMPL_'+str(year)+'_season1'] \n",
    "        \n",
    "        emi = emissivity_array[str(year)]\n",
    "        lst = lst_array[str(year)]\n",
    "        ndvi = veg_ind_array['ndvi_'+str(year)]\n",
    "        evi = veg_ind_array['evi_'+str(year)]\n",
    "        vi = veg_ind_array['vi_'+str(year)]\n",
    "        \n",
    "    \n",
    "    # distance to dense populated areas\n",
    "    dist_2 = dist_density_array['2']\n",
    "    dist_3 = dist_density_array['3']\n",
    "    dist_4 = dist_density_array['4']\n",
    "    \n",
    "    #topographic variables\n",
    "    df_tmp = pd.DataFrame(data=np.c_[land_cover, slope_array, dem_array, protected_array, aspect_array, hillshade_array, \n",
    "                                    fireyear_array_bin[str(year)], qflag, minv, maxv, rslope, lslope, ampl, \n",
    "                                     dist_2, dist_3, dist_4, fireyear_array_bin['2001-2005'], \n",
    "                                  fireyear_array_bin['2012-2017'], fireyear_array_bin['2018'], \n",
    "                                  fireyear_array_bin['2019'], fireyear_array_bin['2020'], fireyear_array_bin['2021'], \n",
    "                                    fireyear_array_bin['2022'], urb, lst, ndvi, evi, vi, emi])\n",
    "    \n",
    "    # rename columns by their attribute name\n",
    "    df_tmp.columns = ['lc', 'slope', 'dem', 'protected', 'aspect', 'hillshade', 'is_fire', 'qflag', 'minv', 'maxv', 'rslope', 'lslope', 'ampl',\n",
    "                     'dist_2', 'dist_3', 'dist_4', 'F/2001-2005', 'F/2012-2017', 'F/2018', 'F/2019', 'F/2020', 'F/2021', \n",
    "                     'F/2022', 'urb', 'lst', 'ndvi', 'evi', 'vi', 'emi']\n",
    "    \n",
    "    # fire count \n",
    "    df_tmp['year'] = int(year[-4:]) if ('-' in year) else int(year)\n",
    "    df_tmp['fire_count'] = df_tmp.apply(lambda x: fire_count_year(x), axis = 1)\n",
    "    df_tmp['fire_count'] = df_tmp['fire_count'].fillna(0)\n",
    "    drop_columns = ['F/2001-2005', 'F/2012-2017', 'F/2018', 'F/2019', 'F/2020', 'F/2021']\n",
    "    df_tmp = df_tmp.drop(columns=drop_columns)\n",
    "    \n",
    "    # Setting land cover type as dummy variable\n",
    "    dummies = pd.get_dummies(df_tmp.lc, prefix='lc')\n",
    "    df_tmp[dummies.columns] = dummies\n",
    "    missing_column = (set(lc_cols) - set(dummies.columns))\n",
    "    for col in list(missing_column):\n",
    "        df_tmp[col] = pd.Series(np.zeros((len(df_tmp))))\n",
    "        \n",
    "    '''# Separating data into blocks\n",
    "    if (y == 0):\n",
    "        block_index = block_index_matrix(h_block_size, w_block_size, width, height)\n",
    "        block_index_array = block_index.reshape(len(df_data))\n",
    "        index = df_tmp.index\n",
    "        df_tmp['block'] = block_index_array[list(index)]\n",
    "    else:\n",
    "        df_tmp['block'] = (df_data_year[FIRE_YEAR[y-1]].block % 5) + 1\n",
    "        df_tmp['block'] = df_tmp['block'].fillna(0)'''\n",
    "    \n",
    "    # filter out values that are outside of the Pyrénées region (i.e all attributes equal to a NO DATA)\n",
    "    df_tmp = df_tmp[(df_tmp.slope != 255) & (df_tmp.dem != 0.) & (df_tmp.lc != 0) & (df_tmp.lc != 1) & (df_tmp.lc != 2) &\n",
    "                (df_tmp.lc != 30) & (df_tmp.lc != 31) & (df_tmp.lc != 32) & (df_tmp.lc != 33)\n",
    "               & (df_tmp.lc != 34) & (df_tmp.minv != -32768) & (df_tmp.maxv != -32768)]\n",
    "    \n",
    "    # update dictionary of dataframes depending on the year\n",
    "    df_data_year.update({str(year): df_tmp})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_data_year['2022']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial CV\n",
    "\n",
    "Spatial cross-validation is a type of cross-validation technique that is used when evaluating machine learning models on spatial data, such as data that includes geographic coordinates or other spatial features. In spatial cross-validation, the data is divided into folds, or subsets, in a way that takes into account the spatial relationships between the data points.\n",
    "\n",
    "The purpose of spatial cross-validation is to ensure that the model is evaluated on a diverse set of spatial configurations, rather than just on a random subset of the data. This is important because spatial patterns and relationships can have a significant impact on the performance of machine learning models, and it is important to ensure that the model is able to generalize to a wide range of spatial configurations.\n",
    "\n",
    "We start by dividing the rasters into blocks of size h_block_size, w_block_size and assigning an index to each of those generated blocks such that the block index is contained in [1, k_fold - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a raster to visualize the block separation we used to perform spatial cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def block_index_matrix(h_block_size, w_block_size, width, height):\n",
    "    k_fold = 5\n",
    "    count = 0\n",
    "    for y in range(int(height/h_block_size)):\n",
    "        for i in range(0, int(width/w_block_size)):\n",
    "            cur = (np.zeros((h_block_size, w_block_size)) + random.randint(1, 5))\n",
    "            if (i != 0):\n",
    "                tmp = np.concatenate((tmp, cur), axis = 1)\n",
    "            else:\n",
    "                tmp = cur\n",
    "            count += 1\n",
    "\n",
    "        tmp = np.concatenate((tmp, (np.zeros((h_block_size, width % w_block_size)) + random.randint(1, 5))), axis=1)\n",
    "        count += 1\n",
    "        if (y == 0):\n",
    "            block_index = tmp\n",
    "        else:\n",
    "            block_index = np.concatenate((block_index, tmp), axis = 0)\n",
    "\n",
    "    block_index = np.concatenate((block_index, np.zeros((height % h_block_size, width))))\n",
    "    return block_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_block_size = 101\n",
    "w_block_size = 129\n",
    "width = 1943\n",
    "height = 1156\n",
    "    \n",
    "block_index = block_index_matrix(h_block_size, w_block_size, width, height)\n",
    "block_index_array = block_index.reshape(len(df_data))\n",
    "\n",
    "df['block'] = block_index_array[list(df.index)]\n",
    "\n",
    "block_values_filter = pd.Series(np.zeros(len(block_index_array)))\n",
    "block_values_filter[df.index] = df['block']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal, osr\n",
    "\n",
    "def visualize_raster(block_values_filter):\n",
    "    ds = gdal.Open('data_final/protected/raster_natural_protected.tif')\n",
    "    band = ds.GetRasterBand(1)\n",
    "    ds.GetProjection()\n",
    "    geotransform = ds.GetGeoTransform()\n",
    "    wkt = ds.GetProjection()\n",
    "\n",
    "\n",
    "    # Create gtif file\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    output_file = \"output/block_cv.tif\"\n",
    "\n",
    "    array_block = np.reshape(block_values_filter, block_index.shape)\n",
    "\n",
    "    dst_ds = driver.Create(output_file,\n",
    "                           band.XSize,\n",
    "                           band.YSize,\n",
    "                           1,\n",
    "                           gdal.GDT_Float32)\n",
    "\n",
    "    #writing output raster\n",
    "    dst_ds.GetRasterBand(1).WriteArray( array_block )\n",
    "\n",
    "    #setting nodata value\n",
    "    dst_ds.GetRasterBand(1).SetNoDataValue(0)\n",
    "\n",
    "    #setting extension of output raster\n",
    "    dst_ds.SetGeoTransform(geotransform)\n",
    "\n",
    "    # setting spatial reference of output raster\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromWkt(wkt)\n",
    "    dst_ds.SetProjection( srs.ExportToWkt() )\n",
    "\n",
    "    #Close output raster dataset\n",
    "    ds = None\n",
    "    dst_ds = None\n",
    "    \n",
    "    block_raster = rasterio.open('output/block_cv.tif')\n",
    "\n",
    "    fig, axs = plt.subplots(1, 1, figsize = (21,7))\n",
    "    show((block_raster, 1), ax=axs, cmap = 'Reds', title = 'Block indexing spatial cross-validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_raster(block_values_filter.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to perform spatial k-fold cross-validation i.e the data is divided into k folds, and the model is trained and evaluated on k-1 folds, with one fold left out as the test set. We visualize how the group k-fold would merge our groups with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "\n",
    "gkf_cv = StratifiedGroupKFold(n_splits=5)\n",
    "feature_norm = ['slope', 'dem', 'emi', 'lst', 'ndvi', 'evi', 'vi', 'qflag', 'ampl', 'minv', 'maxv', 'rslope', 'lslope', \n",
    "                'dist_2', 'dist_3', 'dist_4', 'fire_count']\n",
    "feature_cols = ['protected'] + lc_cols + feature_norm\n",
    "\n",
    "for split, (ix_train, ix_test) in enumerate(gkf_cv.split(df.loc[:, feature_cols], df.is_fire, groups = df['block'])):\n",
    "    block_values_filter[df.index] = df['block']\n",
    "    block_values_filter[df.index[ix_train]] = 3\n",
    "    block_values_filter[df.index[ix_test]] = 5\n",
    "    df_split_train = (df.loc[df.index[ix_train]])\n",
    "    df_split_test = (df.loc[df.index[ix_test]])\n",
    "    print('Split', split)\n",
    "    print('Ratio train : ', len(df_split_train[df_split_train.is_fire == 1])/len(df_split_train))\n",
    "    print('Ratio test : ', len(df_split_test[df_split_test.is_fire == 1])/len(df_split_test))\n",
    "    visualize_raster(block_values_filter.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "feature_norm = ['slope', 'dem', 'emi', 'lst', 'ndvi', 'evi', 'vi', 'qflag', 'ampl', 'minv', 'maxv', 'rslope', 'lslope', \n",
    "                'dist_2', 'dist_3', 'dist_4', 'fire_count']\n",
    "\n",
    "# min max normalization\n",
    "df[feature_norm] = (df[feature_norm] - df[feature_norm].min()) / (df[feature_norm].max() - df[feature_norm].min())\n",
    "df_test[feature_norm] = (df_test[feature_norm] - df_test[feature_norm].min()) / (df_test[feature_norm].max() - df_test[feature_norm].min())\n",
    "\n",
    "index = df.index\n",
    "df_ind = df.reset_index()\n",
    "feature_cols = ['protected'] + lc_cols + feature_norm\n",
    "X_train = df_ind.loc[:, feature_cols]\n",
    "y_train = df_ind.is_fire\n",
    "\n",
    "X_test = df_test.loc[:, feature_cols]\n",
    "y_test = df_test.is_fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle('X_train.pkl')\n",
    "X_test.to_pickle('X_test.pkl')\n",
    "y_train.to_pickle('y_train.pkl')\n",
    "y_test.to_pickle('y_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calibration**  \n",
    "Probability calibration is the process of adjusting the predicted probabilities output by a machine learning model such that they accurately reflect the true underlying probabilities of the classes. In other words, it involves adjusting the model's predictions so that they are more closely aligned with the actual outcomes of the events being predicted.\n",
    "\n",
    "There are two main types of probability calibration: Platt scaling and isotonic regression. Platt scaling, also known as sigmoid calibration, involves fitting a sigmoid curve to the predicted probabilities output by the model, in order to adjust them to be more accurate. Isotonic regression involves fitting a step function to the predicted probabilities, such that the probabilities are monotonically increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model + calibration\n",
    "cv = ((train, test) for train, test in StratifiedGroupKFold(n_splits=5).split(X_train, y_train, groups=df['block']))\n",
    "rf_clf = RandomForestClassifier(max_depth = 3, class_weight = 'balanced', n_estimators = 100)\n",
    "calibrated_clf = CalibratedClassifierCV(rf_clf, method = 'isotonic', cv = cv)\n",
    "calibrated_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = calibrated_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score, fbeta_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def make_classification_report(y_test, y_predict, threshold = 0):\n",
    "    y_pred = y_predict\n",
    "    if (threshold != 0):\n",
    "        y_pred = [1 if (x >= threshold) else 0 for x in y_predict]\n",
    "    # precision\n",
    "    print('Precision', precision_score(y_test, y_pred))\n",
    "    # recall \n",
    "    print('Recall', recall_score(y_test, y_pred))\n",
    "    # f1-score is the harmonic mean between precision and recall\n",
    "    print('F1-Score', f1_score(y_test, y_pred))\n",
    "    # case where false negative (non-detected fire is more costly than a false positive)\n",
    "    print('F2-Score', fbeta_score(y_test, y_pred, beta = 2))\n",
    "    print('Accuracy', accuracy_score(y_test, y_pred))\n",
    "    print('Classification Report : ')\n",
    "    print(classification_report(y_test, y_pred, target_names=['Not burnt', 'Burnt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_classification_report(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics \n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "def plot_metrics(y_test, y_pred_proba, y_predict):\n",
    "    #define metrics\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "    lr_precision, lr_recall, _ = metrics.precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "    f, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "    #create ROC curve\n",
    "    axs[0].plot(fpr, tpr, label=\"AUC=\"+str(auc))\n",
    "    axs[0].plot(lr_precision, lr_recall, label=\"Precision/Recall\")\n",
    "    axs[0].set_ylabel('True Positive Rate/Precision')\n",
    "    axs[0].set_xlabel('False Positive Rate/Recall')\n",
    "    axs[0].legend(loc=4)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay.from_estimator(model, X_test, y_test, ax=axs[1], colorbar=False)\n",
    "    _ = disp.ax_.set_title(\"Calibrated Random Forest\")\n",
    "    plt.show()\n",
    "    return fpr, tpr, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, thresholds = plot_metrics(y_test, y_pred_proba, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the optimal threshold as the point of the ROC curve closest to the top left-corner (i.e true positive rate equal to 1 and false positive rate equal to 0). Mathematically, we write it as :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal threshold for the classifier \n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(\"Threshold value is:\", optimal_threshold)\n",
    "\n",
    "make_classification_report(y_test, y_pred_proba, optimal_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Susceptibility map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_susceptibility_map(filename, y_pred_proba):\n",
    "    ds = gdal.Open('data_final/protected/raster_natural_protected.tif')\n",
    "    band = ds.GetRasterBand(1)\n",
    "    ds.GetProjection()\n",
    "    geotransform = ds.GetGeoTransform()\n",
    "    wkt = ds.GetProjection()\n",
    "\n",
    "\n",
    "    # Create gtif file\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    output_file = \"output/\"+filename+\".tif\"\n",
    "\n",
    "    y_pred_rescaled = pd.Series(np.zeros(len(df_data)))\n",
    "    y_pred_rescaled[X_test.index] = y_pred_proba\n",
    "    array_rf = np.reshape(y_pred_rescaled.values, (1156, 1943))\n",
    "\n",
    "    dst_ds = driver.Create(output_file,\n",
    "                           band.XSize,\n",
    "                           band.YSize,\n",
    "                           1,\n",
    "                           gdal.GDT_Float32)\n",
    "\n",
    "    #writing output raster\n",
    "    dst_ds.GetRasterBand(1).WriteArray( array_rf )\n",
    "\n",
    "    #setting nodata value\n",
    "    dst_ds.GetRasterBand(1).SetNoDataValue(0)\n",
    "\n",
    "    #setting extension of output raster\n",
    "    dst_ds.SetGeoTransform(geotransform)\n",
    "\n",
    "    # setting spatial reference of output raster\n",
    "    srs = osr.SpatialReference()\n",
    "    srs.ImportFromWkt(wkt)\n",
    "    dst_ds.SetProjection( srs.ExportToWkt() )\n",
    "\n",
    "    #Close output raster dataset\n",
    "    ds = None\n",
    "    dst_ds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_susceptibility_map('susceptibility_rf', y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "susceptibility_rf = rasterio.open('output/susceptibility_rf.tif')\n",
    "array_r = susceptibility_rf.read(1).flatten()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(21,7))\n",
    "show((susceptibility_rf, 1), ax=axs[0], cmap='Reds', title = 'Susceptibility Map 2022 using Random Forest')\n",
    "show((fireyear_raster['2022'], 1), cmap='Reds', ax=axs[1], title = 'Fire Map 2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-calibration RandomForest feature importance** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-calibrated model\n",
    "precalib_rf = model.base_estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = precalib_rf.feature_importances_\n",
    "\n",
    "feature_imp = pd.DataFrame({'Value': importances, 'Feature': [legend_column[s] if ('lc' in s) else s for s in list(X_train.columns)]})\n",
    "\n",
    "plt.figure(figsize=(32, 11))\n",
    "sn.set(font_scale = 2)\n",
    "sn.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n",
    "                                                        ascending=False)[0:10])\n",
    "plt.title('RandomForest Features Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/rf_importances-01.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SHAP values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "# using calibrated classifier base estimator\n",
    "sample_test = shap.sample(X_test, 100) \n",
    "sample_test = sample_test.reset_index().drop(columns = ['index'])\n",
    "sample_test.columns = [legend_column[s] if ('lc' in s) else s for s in list(sample_test.columns)]\n",
    "\n",
    "size = len(sample_test)\n",
    "explainer = shap.TreeExplainer(precalib_rf)\n",
    "shap_values = explainer.shap_values(sample_test)\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][:size,:], sample_test.iloc[:size,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, sample_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV, CalibrationDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "feature_norm = ['slope', 'dem', 'emi', 'lst', 'ndvi', 'evi', 'vi', 'qflag', 'ampl', 'minv', 'maxv', 'rslope', 'lslope', \n",
    "                'dist_2', 'dist_3', 'dist_4', 'fire_count']\n",
    "\n",
    "# min max normalization\n",
    "df[feature_norm] = (df[feature_norm] - df[feature_norm].min()) / (df[feature_norm].max() - df[feature_norm].min())\n",
    "df_test[feature_norm] = (df_test[feature_norm] - df_test[feature_norm].min()) / (df_test[feature_norm].max() - df_test[feature_norm].min())\n",
    "\n",
    "index = df.index\n",
    "df_ind = df.reset_index()\n",
    "feature_cols = ['protected'] + lc_cols + feature_norm\n",
    "\n",
    "X_train = df_ind.loc[:, feature_cols]\n",
    "y_train = df_ind.is_fire\n",
    "\n",
    "X_test = df_test.loc[:, feature_cols]\n",
    "y_test = df_test.is_fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.metrics import log_loss\n",
    "import lightgbm as lgb\n",
    "\n",
    "\"\"\"# model + calibration \n",
    "for train, test in StratifiedGroupKFold(n_splits=5).split(X_train, y_train, groups=df['block']):\n",
    "    X_train_fold = (X_train.loc[X_train.index[train]])\n",
    "    y_train_fold = (y_train.loc[y_train.index[train]])\n",
    "    \n",
    "    X_test_fold = (X_test.loc[X_test.index[test]])\n",
    "    y_test_fold = (y_test.loc[y_test.index[test]])\n",
    "    \n",
    "    # model training\n",
    "    rf_clf = lgb.LGBMClassifier(boosting='goss', max_depth=6)\n",
    "    calibrated_clf = CalibratedClassifierCV(rf_clf, method = 'isotonic')\n",
    "    calibrated_clf.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # evaluation\n",
    "    y_pred = calibrated_clf.predict(X_test_fold)\n",
    "    print(\"LOG LOSS :\", log_loss(y_test_fold, y_pred))\"\"\"\n",
    "    \n",
    "# model training\n",
    "rf_clf = lgb.LGBMClassifier(boosting='goss', max_depth=20, n_estimators=100)\n",
    "calibrated_clf = CalibratedClassifierCV(rf_clf, method = 'isotonic')\n",
    "calibrated_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = calibrated_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_classification_report(y_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict_proba(X_test)[::,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot_metrics(y_test, y_pred_proba, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_susceptibility_map('sus_lgbm', y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "susceptibility_rf = rasterio.open('output/sus_lgbm.tif')\n",
    "array_r = susceptibility_rf.read(1).flatten()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(21,7))\n",
    "show((susceptibility_rf, 1), ax=axs[0], cmap='Reds', title = 'Susceptibility Map 2022 using LightGBM')\n",
    "show((fireyear_raster['2022'], 1), ax=axs[1], title = 'Fire Map 2022')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explainability\n",
    "**Pre-calibration LightGBM feature importance** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-calibrated model\n",
    "lgbm_best = model.base_estimator.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-calibration model\n",
    "booster = lgbm_best.booster_\n",
    "feature_imp = pd.DataFrame({'Value': booster.feature_importance(), 'Feature': [legend_column[s] if ('lc' in s) else s for s in list(X_train.columns)]})\n",
    "\n",
    "plt.figure(figsize=(32, 11))\n",
    "sn.set(font_scale = 2)\n",
    "sn.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", \n",
    "                                                        ascending=False)[0:10])\n",
    "plt.title('LightGBM Features Importance')\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/lgbm_importances-01.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-calibration SHAP values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "# using calibrated classifier base estimator\n",
    "sample_test = shap.sample(X_test, 100) \n",
    "sample_test = sample_test.reset_index().drop(columns = ['index'])\n",
    "sample_test.columns = [legend_column[s] if ('lc' in s) else s for s in list(sample_test.columns)]\n",
    "\n",
    "size = len(sample_test)\n",
    "explainer = shap.TreeExplainer(lgbm_best)\n",
    "shap_values = explainer.shap_values(sample_test)\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[1][:size,:], sample_test.iloc[:size,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values[0], sample_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
